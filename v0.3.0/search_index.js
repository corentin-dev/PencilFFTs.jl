var documenterSearchIndex = {"docs":
[{"location":"examples/gradient/#Gradient-of-a-scalar-field-1","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"","category":"section"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"This example shows different methods to compute the gradient of a real-valued 3D scalar field θ(bmx) in Fourier space, where bmx = (x y z). It is assumed that the field is periodic with period L = 2π along all dimensions.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"A working implementation of this example can be found in examples/gradient.jl.","category":"page"},{"location":"examples/gradient/#General-procedure-1","page":"Gradient of a scalar field","title":"General procedure","text":"","category":"section"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"The discrete Fourier expansion of θ writes","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"θ(bmx) = _bmk  Z^3 hatθ(bmk)  e^i bmk  bmx","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"where bmk = (k_x k_y k_z) are the Fourier wave numbers and hatθ is the discrete Fourier transform of θ. Then, the spatial derivatives of θ are given by","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"frac θ(bmx) x_i =\n_bmk  Z^3 i k_i hatθ(bmk)  e^i bmk  bmx","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"where the underscript i denotes one of the spatial components x, y or z.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"In other words, to compute bm θ = (_x θ _y θ _z θ), one has to:","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"transform θ to Fourier space to obtain hatθ,\nmultiply hatθ by i bmk,\ntransform the result back to physical space to obtain bm θ.","category":"page"},{"location":"examples/gradient/#Preparation-1","page":"Gradient of a scalar field","title":"Preparation","text":"","category":"section"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"In this section, we initialise a random real-valued scalar field θ and compute its FFT. For more details see the Tutorial.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"using MPI\nusing PencilFFTs\nusing Random\n\nMPI.Init()\n\n# Input data dimensions (Nx × Ny × Nz)\ndims = (16, 32, 64)\n\n# Apply a 3D real-to-complex (r2c) FFT.\ntransform = Transforms.RFFT()\n\n# MPI topology information\ncomm = MPI.COMM_WORLD  # we assume MPI.Comm_size(comm) == 12\nproc_dims = (3, 4)     # 3 processes along `y`, 4 along `z`\nproc_dims = (1, 1) # (the example actually runs on 1 process...) # hide\n\n# Create plan\nplan = PencilFFTPlan(dims, transform, proc_dims, comm)\n\n# Allocate data and initialise field\nθ = allocate_input(plan)\nrandn!(θ)\n\n# Perform distributed FFT\nθ_hat = plan * θ\n\n# Finally, we initialise the output that will hold ∇θ in Fourier space.\n# Noting that ∇θ is a vector field, we choose to store it as a tuple of\n# 3 PencilArrays.\n# These two are exactly equivalent:\n# ∇θ_hat = ntuple(d -> similar(θ_hat), Val(3))\n∇θ_hat = allocate_output(plan, Val(3))\nnothing # hide","category":"page"},{"location":"examples/gradient/#Fourier-wave-numbers-1","page":"Gradient of a scalar field","title":"Fourier wave numbers","text":"","category":"section"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"In general, the Fourier wave numbers are of the form k_i = 0 frac2πL_i frac4πL_i frac6πL_i ldots, where L_i is the period along dimension i. When a real-to-complex Fourier transform is applied, roughly half of these wave numbers are redundant due to the Hermitian symmetry of the complex Fourier coefficients. In practice, this means that for the fastest dimension x (along which a real-to-complex transform is performed), the negative wave numbers are dropped, i.e. k_x = 0 frac2πL_x frac4πL_x ldots.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"The AbstractFFTs package provides a convenient way to generate the Fourier wave numbers, using the functions fftfreq and rfftfreq. We can use these functions to initialise a \"grid\" of wave numbers associated to our 3D real-to-complex transform:","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"using AbstractFFTs: fftfreq, rfftfreq\n\nbox_size = (2π, 2π, 2π)  # Lx, Ly, Lz\nsample_rate = 2π .* dims ./ box_size\n\n# In our case (Lx = 2π and Nx even), this gives kx = [0, 1, 2, ..., Nx/2].\nkx = rfftfreq(dims[1], sample_rate[1])\n\n# In our case (Ly = 2π and Ny even), this gives\n# ky = [0, 1, 2, ..., Ny/2-1, -Ny/2, -Ny/2+1, ..., -1] (and similarly for kz).\nky = fftfreq(dims[2], sample_rate[2])\nkz = fftfreq(dims[3], sample_rate[3])\n\nkvec = (kx, ky, kz)\nnothing # hide","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"Note that kvec now contains the wave numbers associated to the global domain. In the following, we will only need the wave numbers associated to the portion of the domain handled by the local MPI process.","category":"page"},{"location":"examples/gradient/#gradient_method_global-1","page":"Gradient of a scalar field","title":"Method 1: global views","text":"","category":"section"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"PencilArrays, returned for instance by allocate_input and  allocate_output, take indices that start at 1, regardless of the location of the subdomain associated to the local process on the global grid. (We say that PencilArrays take local indices.) On the other hand, we have defined the wave number vector kvec which, for each MPI process, is defined over the global domain, and as such it takes global indices.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"One straightforward way of making data arrays compatible with wave numbers is to use global views, i.e. arrays that take global indices. These are generated from PencilArrays by calling the global_view function. Note that, in general, global indices do not start at 1 for a given MPI process. A given process will own a range of data given by indices in (i1:i2, j1:j2, k1:k2).","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"Once we have global views, we can combine data and wave numbers using the portion of global indices owned by the local MPI process, as shown below.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"# Generate global views of PencilArrays.\nθ_glob = global_view(θ_hat)\n∇θ_glob = global_view.(∇θ_hat)  # we broadcast over the 3 elements of ∇θ_hat\n\n# We can use CartesianIndices to iterate over the global indices associated to\n# the local process.\nfor I in CartesianIndices(θ_glob)\n    i, j, k = Tuple(I)  # unpack indices\n\n    # Wave number vector associated to current Cartesian index.\n    kx = kvec[1][i]\n    ky = kvec[2][j]\n    kz = kvec[3][k]\n\n    # Compute gradient in Fourier space.\n    # Note that modifying ∇θ_glob also modifies the original PencilArray ∇θ_hat.\n    ∇θ_glob[1][I] = im * kx * θ_glob[I]\n    ∇θ_glob[2][I] = im * ky * θ_glob[I]\n    ∇θ_glob[3][I] = im * kz * θ_glob[I]\nend","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"The above loop can be written in a more efficient manner by precomputing im * θ_glob[I] and by avoiding indexation with the CartesianIndex I, using linear indexing instead:[1]","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"@inbounds for (n, I) in enumerate(CartesianIndices(θ_glob))\n    i, j, k = Tuple(I)\n\n    kx = kvec[1][i]\n    ky = kvec[2][j]\n    kz = kvec[3][k]\n\n    u = im * θ_glob[n]\n\n    ∇θ_glob[1][n] = kx * u\n    ∇θ_glob[2][n] = ky * u\n    ∇θ_glob[3][n] = kz * u\nend","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"This is basically the implementation of gradient_global_view! in examples/gradient.jl. Also note that the above can be easily written in a more generic way, e.g. for arbitrary dimensions, thanks in part to the use of CartesianIndices. Moreover, in the above there is no notion of the dimension permutations discussed in the tutorial, as it is all hidden behind the implementation of PencilArrays.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"Finally, we can perform a backwards transform to obtain bm θ in physical space:","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"∇θ = plan \\ ∇θ_hat\nnothing # hide","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"Note that the transform is automatically broadcast over the three fields of the ∇θ_hat vector, and the result ∇θ is also a tuple of three PencilArrays.","category":"page"},{"location":"examples/gradient/#gradient_method_global_explicit-1","page":"Gradient of a scalar field","title":"Method 2: explicit global indexing","text":"","category":"section"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"Sometimes, one does not need to write generic code. In our case, one often knows the dimensionality of the problem and the memory layout of the data (i.e. the underlying index permutation).","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"Below is a reimplementation of the above loop, using explicit indices instead of CartesianIndices, and assuming that the underlying index permutation is (3, 2, 1), that is, data is stored in (z y x) order. As discussed in the tutorial, this is the default for transformed arrays. This example also serves as a clearer explanation for what is going on in the first method.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"# Get local data range in the global grid.\nrng = axes(θ_glob)  # = (i1:i2, j1:j2, k1:k2)\n\n# For the loop below, we're assuming that the permutation is (3, 2, 1).\n# In other words, the fastest index is the *last* one, and not the first one as\n# it is usually in Julia.\n# If the permutation is not (3, 2, 1), things will still work (well, except for\n# the assertion below!), but the loop order will not be optimal.\n@assert get_permutation(θ_hat) === Val((3, 2, 1))\n\n@inbounds for i in rng[1], j in rng[2], k in rng[3]\n    kx = kvec[1][i]\n    ky = kvec[2][j]\n    kz = kvec[3][k]\n\n    # Note that we still access the arrays in (i, j, k) order.\n    # (The permutation happens behind the scenes!)\n    u = im * θ_glob[i, j, k]\n\n    ∇θ_glob[1][i, j, k] = kx * u\n    ∇θ_glob[2][i, j, k] = ky * u\n    ∇θ_glob[3][i, j, k] = kz * u\nend","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"This implementation corresponds to gradient_global_view_explicit! in examples/gradient.jl. Perhaps surprisingly, this implementation of the gradient is the fastest of all tested. (The \"local index\" implementation below is about 20% slower, while Method 1 is 50% slower.) Note that we don't even need to switch to linear indexing to obtain optimal performance! Apparently there's a lot of compiler optimisations going on specifically for this function. This is evident when running julia with the -O1 optimisation level (the default is -O2), in which case this implementation becomes much slower than the others (tested with Julia 1.3.1).","category":"page"},{"location":"examples/gradient/#gradient_method_local-1","page":"Gradient of a scalar field","title":"Method 3: using local indices","text":"","category":"section"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"Alternatively, we can avoid global views and work directly on PencilArrays using local indices that start at 1. In this case, part of the strategy is to construct a \"local\" grid of wave numbers that can also be accessed with local indices. Moreover, to obtain the local data range associated to a PencilArray, we call the range_local function. Apart from these details, this method is very similar to the first one.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"# Get local data range in the global grid.\nrng = range_local(θ_hat)  # = (i1:i2, j1:j2, k1:k2)\n\n# Local wave numbers: (kx[i1:i2], ky[j1:j2], kz[k1:k2]).\nkvec_local = ntuple(d -> kvec[d][rng[d]], Val(3))\n\n# For clarity, the above can also be expressed as:\n# kx_vec = kvec[1][rng[1]]  # kx[i1:i2]\n# ky_vec = kvec[2][rng[2]]  # ky[j1:j2]\n# kz_vec = kvec[3][rng[3]]  # kz[k1:k2]\n# kvec_local = (kx_vec, ky_vec, kz_vec)\n\n@inbounds for (n, I) in enumerate(CartesianIndices(θ_hat))\n    i, j, k = Tuple(I)  # local indices\n\n    # Wave number vector associated to current Cartesian index.\n    kx = kvec_local[1][i]\n    ky = kvec_local[2][j]\n    kz = kvec_local[3][k]\n\n    u = im * θ_hat[n]\n\n    ∇θ_hat[1][n] = kx * u\n    ∇θ_hat[2][n] = ky * u\n    ∇θ_hat[3][n] = kz * u\nend","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"This implementation corresponds to gradient_local! in examples/gradient.jl. Like Method 1, this implementation uses CartesianIndices and can be made more generic with little effort. In particular, there is no explicit use of index permutations, and no assumptions need to be made in that regard. In our tests, this implementation is about 20% faster than Method 1, while still being generic and almost equally simple.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"In examples/gradient.jl, additional implementations using local indices can be found which rely on a more advanced understanding of permutations and on the internals of the Pencils module. See for instance gradient_local_parent!, which directly works with the raw data stored in Julia Arrays; or gradient_local_linear!, which completely avoids CartesianIndices while staying generic and efficient. We have found that these display roughly the same performance as the example above.","category":"page"},{"location":"examples/gradient/#Summary-1","page":"Gradient of a scalar field","title":"Summary","text":"","category":"section"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"The Pencils module provides different alternatives to deal with MPI-distributed data that may be subject to dimension permutations. In particular, one can choose to work with global indices (first two examples) or with local indices (third example).","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"If one wants to stay generic, making sure that the same code will work for arbitrary dimensions and will be efficient regardless of the underlying dimension permutation, methods 1 and 3 should be preferred. These use CartesianIndices and make no assumptions on the permutations (actually, permutations are completely invisible in the implementations). Method 3 is faster and should be preferred for performance.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"The second method uses explicit (i, j, k) indices. It assumes that the underlying permutation is (3, 2, 1) to loop with i as the slowest index and k as the fastest, which is the optimal order in this case given the permutation. As such, the implementation is less generic than the others, but is slightly easier to read. Surprisingly, it achieves better performance than the other methods (about 20% faster than Method 3), possibly because the compiler can make further assumptions at the optimisation stage.","category":"page"},{"location":"examples/gradient/#","page":"Gradient of a scalar field","title":"Gradient of a scalar field","text":"[1]: This assumes that CartesianIndices(θ_glob) iterates in the order of the array elements in memory. This is not trivial when the array dimensions are permuted (which is the default for transformed arrays in PencilFFTs), and it actually wasn't the case until commit 843fe42 (or PencilFFTs v0.2.0).","category":"page"},{"location":"benchmarks/#Benchmarks-1","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks/#","page":"Benchmarks","title":"Benchmarks","text":"The performance of PencilFFTs.jl is comparable to that of other open-source parallel FFT libraries implemented in lower-level languages. Below, we show comparisons with the Fortran implementation of P3DFFT, possibly the most popular of these libraries. The benchmarks were performed on the Jean–Zay cluster of the IDRIS French computing centre (description in English).","category":"page"},{"location":"benchmarks/#","page":"Benchmarks","title":"Benchmarks","text":"The figure below shows strong scaling benchmarks of 3D real-to-complex FFTs using 2D (\"pencil\") decomposition. The benchmarks were run for input arrays of dimensions N_x  N_y  N_z = 512^3 and 1024^3. Each timing is averaged over 100 repetitions.","category":"page"},{"location":"benchmarks/#","page":"Benchmarks","title":"Benchmarks","text":"<div class=\"figure\">\n  <!--\n  Note: this is evaluated from the directory where the Benchmarks page is\n  built. This directory varies depending on whether `prettyurls` is enabled in\n  `makedocs`. Here we assume `prettyurls=true`.\n  -->\n  <img\n    width=\"75%\"\n    src=\"../img/benchmark_idris.svg\"\n    alt=\"Strong scaling of PencilFFTs\">\n</div>","category":"page"},{"location":"benchmarks/#","page":"Benchmarks","title":"Benchmarks","text":"The performance and scalability of PencilFFTs are similar to those displayed by P3DFFT. In general, P3DFFT has a small advantage over PencilFFTs, which is possibly explained because their local array reordering routines are better optimised. This difference is especially important for low process counts, where the relative cost of data reordering is higher. In the future we expect to close that performance gap.","category":"page"},{"location":"benchmarks/#","page":"Benchmarks","title":"Benchmarks","text":"An important difference with P3DFFT is that PencilFFTs uses non-blocking point-to-point MPI communications by default (using MPI_Isend and MPI_Irecv), while P3DFFT uses global MPI_Alltoallv calls. This enables us to perform data reordering operations on the partially received data while we wait for the incoming data. This can lead to better performance especially when running on a large number of processes, in which case the cost of MPI communications is largely dominant.","category":"page"},{"location":"benchmarks/#","page":"Benchmarks","title":"Benchmarks","text":"Note that PencilFFTs can optionally use MPI_Alltoallv instead of point-to-point communications (see the docs for PencilFFTPlan and transpose!). We have verified that the implementation with MPI_Isend and MPI_Irecv generally outperforms the one based on MPI_Alltoallv. Observed performance gains can be of the order of 10%.","category":"page"},{"location":"benchmarks/#Benchmark-details-1","page":"Benchmarks","title":"Benchmark details","text":"","category":"section"},{"location":"benchmarks/#","page":"Benchmarks","title":"Benchmarks","text":"The benchmarks were performed using Julia 1.3.1 and Intel MPI 2019.0.4. We used PencilFFTs v0.2.0 with FFTW.jl v1.2.0 and MPI.jl v0.11.0. We used the Fortran implementation of P3DFFT, version 2.7.6, which was built with Intel 2019 compilers and linked to FFTW 3.3.8. The cluster where the benchmarks were run has Intel Cascade Lake 6248 processors with 2×20 cores per node.","category":"page"},{"location":"benchmarks/#","page":"Benchmarks","title":"Benchmarks","text":"The number of MPI processes along each decomposed dimension, P_1 and P_2, was automatically determined by a call to MPI_Dims_create, which tends to create a balanced decomposition with P_1  P_2. For instance, a total of 1024 processes is divided into P_1 = P_2 = 32. Different results may be obtained with other combinations, but this was not benchmarked.","category":"page"},{"location":"benchmarks/#","page":"Benchmarks","title":"Benchmarks","text":"The source files used to generate this benchmark, as well as the raw benchmark results, are all available in the PencilFFTs repo.","category":"page"},{"location":"Pencils/#Pencils_module-1","page":"MPI-distributed data","title":"MPI-distributed data","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"CurrentModule = PencilFFTs.Pencils","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"The distribution of global data among MPI processes is managed by the Pencils module. This module may be used independently of the FFT functionality.","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"The Pencils module defines types that describe an MPI Cartesian topology and the decomposition of data over MPI processes. The module also defines array wrappers, most notably the PencilArray type, which allow to conveniently and efficiently work with MPI-decomposed data.","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"Pencils","category":"page"},{"location":"Pencils/#PencilFFTs.Pencils","page":"MPI-distributed data","title":"PencilFFTs.Pencils","text":"Module for multidimensional data decomposition using MPI.\n\nHandles different decomposition configurations and data transpositions between them. Also defines relevant data structures for handling distributed data.\n\n\n\n\n\n","category":"module"},{"location":"Pencils/#sec:mpi_topology-1","page":"MPI-distributed data","title":"MPI topology","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"The MPITopology type defines the MPI Cartesian topology of the decomposition. In other words, it contains information about the number of decomposed dimensions, and the number of processes in each of these dimensions.","category":"page"},{"location":"Pencils/#Construction-1","page":"MPI-distributed data","title":"Construction","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"The main MPITopology constructor takes a MPI communicator and a tuple specifying the number of processes in each dimension. For instance, to distribute 12 MPI processes on a 3  4 grid:","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"comm = MPI.COMM_WORLD  # we assume MPI.Comm_size(comm) == 12\npdims = (3, 4)\ntopology = MPITopology(comm, pdims)","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"At the lower level, MPITopology uses MPI_Cart_create to define a Cartesian MPI communicator. For more control, one can also create a Cartesian communicator using MPI.Cart_create, and pass that to MPITopology:","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"comm = MPI.COMM_WORLD\ndims = [3, 4]  # note: array, not tuple!\nperiods = zeros(Int, N)\nreorder = false\ncomm_cart = MPI.Cart_create(comm, dims, periods, reorder)\ntopology = MPITopology(comm_cart)","category":"page"},{"location":"Pencils/#Types-1","page":"MPI-distributed data","title":"Types","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"MPITopology","category":"page"},{"location":"Pencils/#PencilFFTs.Pencils.MPITopologies.MPITopology","page":"MPI-distributed data","title":"PencilFFTs.Pencils.MPITopologies.MPITopology","text":"MPITopology{N}\n\nDescribes an N-dimensional Cartesian MPI decomposition topology.\n\n\n\nMPITopology(comm::MPI.Comm, pdims::Dims{N}) where N\n\nCreate N-dimensional MPI topology information.\n\nThe pdims tuple specifies the number of MPI processes to put in every dimension of the topology. The product of its values must be equal to the number of processes in communicator comm.\n\nExample\n\n# Divide 2D topology into 4×2 blocks.\ncomm = MPI.COMM_WORLD\n@assert MPI.Comm_size(comm) == 8\ntopology = MPITopology(comm, (4, 2))\n\n\n\nMPITopology{N}(comm_cart::MPI.Comm) where N\n\nCreate topology information from MPI communicator with Cartesian topology (typically constructed using MPI.Cart_create). The topology must have dimension N.\n\n\n\n\n\n","category":"type"},{"location":"Pencils/#Methods-1","page":"MPI-distributed data","title":"Methods","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"get_comm(::MPITopology)\nlength(::MPITopology)\nndims(::MPITopology)\nsize(::MPITopology)","category":"page"},{"location":"Pencils/#PencilFFTs.Pencils.MPITopologies.get_comm-Tuple{MPITopology}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.MPITopologies.get_comm","text":"get_comm(t::MPITopology)\n\nGet MPI communicator associated to an MPI Cartesian topology.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.length-Tuple{MPITopology}","page":"MPI-distributed data","title":"Base.length","text":"length(t::MPITopology)\n\nGet total size of Cartesian topology (i.e. total number of MPI processes).\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.ndims-Tuple{MPITopology}","page":"MPI-distributed data","title":"Base.ndims","text":"ndims(p::Pencil)\n\nNumber of spatial dimensions associated to pencil data.\n\nThis corresponds to the total number of dimensions of the space, which includes the decomposed and non-decomposed dimensions.\n\n\n\n\n\nndims(t::MPITopology)\n\nGet dimensionality of Cartesian topology.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.size-Tuple{MPITopology}","page":"MPI-distributed data","title":"Base.size","text":"size(t::MPITopology)\n\nGet dimensions of Cartesian topology.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#sec:pencil_configs-1","page":"MPI-distributed data","title":"Pencil configurations","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"A pencil configuration refers to a given distribution of multidimensional data among MPI processes. This information is encoded in the Pencil type.","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"More precisely, a pencil configuration includes:","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"MPI topology information,\nglobal and local dimensions of the numerical grid,\nsubset of decomposed dimensions,\ntype of decomposed data (e.g. Float64),\ndefinition of optional permutation of dimensions.","category":"page"},{"location":"Pencils/#Construction-2","page":"MPI-distributed data","title":"Construction","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"The creation of a new Pencil requires a MPITopology, as well as the global data dimensions and a list of decomposed dimensions. Optionally, one can also specify the data type (the default is Float64) and a permutation of dimensions.","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"For instance, to decompose along the first and third dimensions of a complex 3D dataset,","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"topology = MPITopology(#= ... =#)\ndims_global = (16, 32, 64)\ndecomp_dims = (1, 3)  # this requires ndims(topology) == 2\npencil = Pencil(topology, dims_global, decomp_dims, Complex{Float64})","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"One may also want to create multiple pencil configurations that differ, for instance, on the selection of decomposed dimensions. For this case, a second constructor is available that takes an already existing Pencil instance. Calling this constructor should be preferred when possible since it allows sharing memory buffers (used for instance for global transpositions) and thus reducing memory usage. The following creates a Pencil equivalent to the one above, but with different decomposed dimensions:","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"pencil_x = Pencil(pencil, decomp_dims=(2, 3))","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"See the Pencil documentation for more details.","category":"page"},{"location":"Pencils/#Dimension-permutations-1","page":"MPI-distributed data","title":"Dimension permutations","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"As mentioned above, a Pencil may optionally be given information on dimension permutations. In this case, the layout of the data arrays in memory is different from the logical order of dimensions.","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"To make this clearer, consider the example above where the global data dimensions are N_x  N_y  N_z = 16  32  64. In this case, the logical order is (x y z). Now let's say that we want the memory order of the data to be (y z x),[1] which corresponds to the permutation (2, 3, 1).","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"Permutations are passed to the Pencil constructor via the permute keyword argument. For performance reasons, in the Pencils module, dimension permutations are compile-time constants, and thus permutations should be specified as value types wrapping a tuple. For instance,","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"permutation = Val((2, 3, 1))\npencil = Pencil(#= ... =#, permute=permutation)","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"One can also pass nothing as a permutation, which disables permutations (this is the default).","category":"page"},{"location":"Pencils/#Types-2","page":"MPI-distributed data","title":"Types","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"Pencil","category":"page"},{"location":"Pencils/#PencilFFTs.Pencils.Pencil","page":"MPI-distributed data","title":"PencilFFTs.Pencils.Pencil","text":"Pencil{N,M,T}\n\nDescribes the decomposition of an N-dimensional Cartesian geometry among MPI processes along M directions (with M < N).\n\nThe Pencil describes the decomposition of arrays of element type T.\n\n\n\nPencil(topology::MPITopology{M}, size_global::Dims{N},\n       decomp_dims::Dims{M}, [element_type=Float64];\n       permute::Union{Nothing, Val}=nothing,\n       timer=TimerOutput())\n\nDefine the decomposition of an N-dimensional geometry along M dimensions.\n\nThe dimensions of the geometry are given by size_global = (N1, N2, ...). The Pencil describes the decomposition of an array of dimensions size_global and type T across a group of MPI processes.\n\nData is distributed over the given M-dimensional MPI topology (with M < N). The decomposed dimensions are given by decomp_dims.\n\nThe optional parameter perm should be a (compile-time) tuple defining a permutation of the data indices. Such permutation may be useful for performance reasons, since it may be preferable (e.g. for FFTs) that the data is contiguous along the pencil direction.\n\nIt is also possible to pass a TimerOutput to the constructor. See Measuring performance for details.\n\nExamples\n\nDecompose a 3D geometry of global dimensions N_x  N_y  N_z = 4812 along the second (y) and third (z) dimensions.\n\nPencil(topology, (4, 8, 12), (2, 3))                          # data is in (x, y, z) order\nPencil(topology, (4, 8, 12), (2, 3), permute=Val((3, 2, 1)))  # data is in (z, y, x) order\n\nIn the second case, the actual data is stored in (z, y, x) order within each MPI process.\n\n\n\nPencil(p::Pencil{N,M}, [element_type=eltype(p)];\n       decomp_dims::Dims{M}=get_decomposition(p),\n       size_global::Dims{N}=size_global(p),\n       permute::P=get_permutation(p),\n       timer::TimerOutput=get_timer(p))\n\nCreate new pencil configuration from an existent one.\n\nThis constructor enables sharing temporary data buffers between the two pencil configurations, leading to reduced global memory usage.\n\n\n\n\n\n","category":"type"},{"location":"Pencils/#Methods-2","page":"MPI-distributed data","title":"Methods","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"eltype(::Pencil)\nget_comm(::Pencil)\nget_decomposition(::Pencil)\nget_permutation(::Pencil)\nlength(::Pencil)\nndims(::Pencil)\nrange_local(::Pencil{N}) where N\nsize_global(::Pencil)\nsize_local(::Pencil)\nto_local(::Pencil)","category":"page"},{"location":"Pencils/#Base.eltype-Tuple{Pencil}","page":"MPI-distributed data","title":"Base.eltype","text":"eltype(Pencil)\neltype(p::Pencil)\n\nElement type associated to the given pencil type.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.MPITopologies.get_comm-Tuple{Pencil}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.MPITopologies.get_comm","text":"get_comm(p::Pencil)\n\nGet MPI communicator associated to an MPI decomposition scheme.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.get_decomposition-Tuple{Pencil}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.get_decomposition","text":"get_decomposition(p::Pencil)\n\nGet tuple with decomposed dimensions of the given pencil configuration.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.get_permutation-Tuple{Pencil}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.get_permutation","text":"get_permutation(p::Pencil)\n\nGet index permutation associated to the given pencil configuration.\n\nReturns nothing if there is no associated permutation.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.length-Tuple{Pencil}","page":"MPI-distributed data","title":"Base.length","text":"length(p::Pencil)\n\nGet linear length of data associated to the local pencil layout.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.ndims-Tuple{Pencil}","page":"MPI-distributed data","title":"Base.ndims","text":"ndims(p::Pencil)\n\nNumber of spatial dimensions associated to pencil data.\n\nThis corresponds to the total number of dimensions of the space, which includes the decomposed and non-decomposed dimensions.\n\n\n\n\n\nndims(t::MPITopology)\n\nGet dimensionality of Cartesian topology.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.range_local-Union{Tuple{Pencil{N,M,T,P} where P where T<:Number where M}, Tuple{N}} where N","page":"MPI-distributed data","title":"PencilFFTs.Pencils.range_local","text":"range_local(p::Pencil; permute=false)\n\nLocal data range held by the pencil.\n\nBy default the dimensions are not permuted, i.e. they follow the logical order of dimensions.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.size_global-Tuple{Pencil}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.size_global","text":"size_global(p::Pencil; permute=false)\n\nGlobal dimensions of the Cartesian grid associated to the given domain decomposition.\n\nLike size_local, by default the returned dimensions are not permuted.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.size_local-Tuple{Pencil}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.size_local","text":"size_local(p::Pencil; permute=false)\n\nLocal dimensions of the data held by the pencil.\n\nBy default the dimensions are not permuted, i.e. they follow the logical order of dimensions.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.to_local-Tuple{Pencil}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.to_local","text":"to_local(p::Pencil, global_inds; permute=false)\n\nConvert non-permuted global indices to local indices.\n\nIndices can be optionally permuted using the permutation associated to the pencil configuration p.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Array-wrappers-1","page":"MPI-distributed data","title":"Array wrappers","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"The Pencils module also defines a PencilArray type that wraps an AbstractArray while including pencil decomposition information.","category":"page"},{"location":"Pencils/#Construction-3","page":"MPI-distributed data","title":"Construction","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"A PencilArray wrapper can be simply constructed from a Pencil instance as","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"pencil = Pencil(#= ... =#)\nA = PencilArray(pencil)\nparent(A)  # returns the allocated Array","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"This allocates a new Array with the local dimensions and data type associated to the Pencil.","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"One can also construct a PencilArray wrapper from an existing AbstractArray, whose dimensions and type must be compatible with the Pencil configuration. For instance, the following works:","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"T = eltype(pencil)\ndims = size_local(pencil, permute=true)  # dimensions of data array must be permuted!\ndata = Array{T}(undef, dims)\nA = PencilArray(pencil, data)","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"Note that data does not need to be a Array, but can be any subtype of AbstractArray.","category":"page"},{"location":"Pencils/#Dimension-permutations-2","page":"MPI-distributed data","title":"Dimension permutations","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"Unlike the wrapped AbstractArray, the PencilArray wrapper takes non-permuted indices. For instance, if the underlying permutation of the Pencil is (2, 3, 1), then A[i, j, k] points to the same value as parent(A)[j, k, i].","category":"page"},{"location":"Pencils/#Global-views-1","page":"MPI-distributed data","title":"Global views","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"PencilArrays are accessed using local indices that start at 1, regardless of the location of the subdomain associated to the local process on the global grid. Sometimes it may be more convenient to use global indices describing the position of the local process in the domain. For this, the global_view function is provided that generates an OffsetArray wrapper taking global indices. For more details, see for instance the gradient example.","category":"page"},{"location":"Pencils/#Types-3","page":"MPI-distributed data","title":"Types","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"PencilArray\nPencilArrayCollection\nMaybePencilArrayCollection\nManyPencilArray","category":"page"},{"location":"Pencils/#PencilFFTs.Pencils.PencilArray","page":"MPI-distributed data","title":"PencilFFTs.Pencils.PencilArray","text":"PencilArray(pencil::Pencil, data::AbstractArray{T,N})\n\nCreate array wrapper with pencil decomposition information.\n\nThe array dimensions and element type must be consistent with those of the given pencil.\n\nnote: Index permutations\nIf the Pencil has an associated index permutation, then data must have its dimensions permuted accordingly.Unlike data, the resulting PencilArray should be accessed with unpermuted indices.ExampleSuppose pencil has local dimensions (10, 20, 30) before permutation, and has an asociated permutation (2, 3, 1). Then:data = zeros(20, 30, 10)       # parent array (with permuted dimensions)\n\nu = PencilArray(pencil, data)  # wrapper with dimensions (10, 20, 30)\n@assert size(u) === (10, 20, 30)\n\nu[15, 25, 5]          # BoundsError (15 > 10 and 25 > 20)\nu[5, 15, 25]          # correct\nparent(u)[15, 25, 5]  # correct\n\n\nnote: Extra dimensions\nThe data array can have one or more extra dimensions to the right (slow indices). For instance, these may correspond to vector or tensor components. These dimensions are not affected by permutations. Support for extra dimensions is experimental and may dissapear in the future!.Exampledims = (20, 30, 10)\nPencilArray(pencil, zeros(dims...))        # works (scalar)\nPencilArray(pencil, zeros(dims..., 3))     # works (3-component vector)\nPencilArray(pencil, zeros(dims..., 4, 3))  # works (4×3 tensor)\nPencilArray(pencil, zeros(3, dims...))     # fails\n\n\n\nPencilArray(pencil::Pencil, [extra_dims=()])\n\nAllocate uninitialised PencilArray that can hold data in the local pencil.\n\nExtra dimensions, for instance representing vector components, can be specified. These dimensions are added to the rightmost (slowest) indices of the resulting array.\n\nExample\n\nSuppose pencil has local dimensions (20, 10, 30) after permutation. Then:\n\nPencilArray(pencil)          # array dimensions are (20, 10, 30)\nPencilArray(pencil, (4, 3))  # array dimensions are (20, 10, 30, 4, 3)\n\n\n\n\n\n","category":"type"},{"location":"Pencils/#PencilFFTs.Pencils.PencilArrayCollection","page":"MPI-distributed data","title":"PencilFFTs.Pencils.PencilArrayCollection","text":"PencilArrayCollection\n\nUnionAll type describing a collection of PencilArrays.\n\nSuch a collection can be a tuple or an array of PencilArrays.\n\nCollections are by assumption homogeneous: each array has the same properties, and in particular, is associated to the same Pencil configuration.\n\nFor convenience, certain operations defined for PencilArray are also defined for PencilArrayCollection, and return the same value as for a single PencilArray. Some examples are pencil, range_local and get_comm.\n\nAlso note that functions from Base, such as size or ndims, are not overloaded for PencilArrayCollection, since they already have a definition for tuples and arrays (and redefining them would be type piracy...).\n\n\n\n\n\n","category":"constant"},{"location":"Pencils/#PencilFFTs.Pencils.MaybePencilArrayCollection","page":"MPI-distributed data","title":"PencilFFTs.Pencils.MaybePencilArrayCollection","text":"MaybePencilArrayCollection\n\nUnionAll type representing either a PencilArray or a collection of PencilArrays.\n\nSee also PencilArrayCollection.\n\n\n\n\n\n","category":"constant"},{"location":"Pencils/#PencilFFTs.Pencils.ManyPencilArray","page":"MPI-distributed data","title":"PencilFFTs.Pencils.ManyPencilArray","text":"ManyPencilArray{T,N,M}\n\nContainer holding M different PencilArray views to the same underlying data buffer. All views share the same element type T and dimensionality N.\n\nThis can be useful to perform in-place operations on PencilArray data.\n\n\n\nManyPencilArray(pencils...; extra_dims=())\n\nCreate a ManyPencilArray container that can hold data associated to all the given Pencils.\n\nThe optional extra_dims argument is the same as for PencilArray.\n\n\n\n\n\n","category":"type"},{"location":"Pencils/#PencilArray-methods-1","page":"MPI-distributed data","title":"PencilArray methods","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"extra_dims(::PencilArray)\nget_comm(::MaybePencilArrayCollection)\nget_permutation(::MaybePencilArrayCollection)\nglobal_view(::PencilArray)\nndims_extra(::MaybePencilArrayCollection)\nndims_space(::PencilArray)\nparent(::PencilArray)\npencil(::PencilArray)\nrange_local(::MaybePencilArrayCollection)\nsize(::PencilArray)\nsize_global(::MaybePencilArrayCollection)","category":"page"},{"location":"Pencils/#PencilFFTs.Pencils.extra_dims-Tuple{PencilArray}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.extra_dims","text":"extra_dims(x::PencilArray)\nextra_dims(x::PencilArrayCollection)\n\nReturn tuple with size of \"extra\" dimensions of PencilArray.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.MPITopologies.get_comm-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.MPITopologies.get_comm","text":"get_comm(x::PencilArray)\nget_comm(x::PencilArrayCollection)\n\nGet MPI communicator associated to a pencil-distributed array.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.get_permutation-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.get_permutation","text":"get_permutation(x::PencilArray)\nget_permutation(x::PencilArrayCollection)\n\nGet index permutation associated to the given PencilArray.\n\nReturns nothing if there is no associated permutation.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.global_view-Tuple{PencilArray}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.global_view","text":"global_view(x::PencilArray)\n\nCreate an OffsetArray of a PencilArray that takes unpermuted global indices.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.ndims_extra-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.ndims_extra","text":"ndims_extra(x::PencilArray)\nndims_extra(x::PencilArrayCollection)\n\nNumber of \"extra\" dimensions associated to PencilArray.\n\nThese are the dimensions that are not associated to the domain geometry. For instance, they may correspond to vector or tensor components.\n\nThese dimensions correspond to the rightmost indices of the array.\n\nThe total number of dimensions of a PencilArray is given by:\n\nndims(x) == ndims_space(x) + ndims_extra(x)\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.ndims_space-Tuple{PencilArray}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.ndims_space","text":"ndims_space(x::PencilArray)\nndims_space(x::PencilArrayCollection)\n\nNumber of dimensions associated to the domain geometry.\n\nThese dimensions correspond to the leftmost indices of the array.\n\nThe total number of dimensions of a PencilArray is given by:\n\nndims(x) == ndims_space(x) + ndims_extra(x)\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.parent-Tuple{PencilArray}","page":"MPI-distributed data","title":"Base.parent","text":"parent(x::PencilArray)\n\nReturn array wrapped by a PencilArray.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.pencil-Tuple{PencilArray}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.pencil","text":"pencil(x::PencilArray)\n\nReturn decomposition configuration associated to a PencilArray.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.range_local-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.range_local","text":"range_local(x::PencilArray; permute=false)\nrange_local(x::PencilArrayCollection; permute=false)\n\nLocal data range held by the PencilArray.\n\nBy default the dimensions are not permuted, matching the order of indices in the array.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.size-Tuple{PencilArray}","page":"MPI-distributed data","title":"Base.size","text":"size(x::PencilArray)\n\nReturn (unpermuted) local dimensions of a PencilArray.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#PencilFFTs.Pencils.size_global-Tuple{Union{PencilArray, Union{Tuple{Vararg{A,N} where N}, AbstractArray{A,N} where N} where A<:PencilArray}}","page":"MPI-distributed data","title":"PencilFFTs.Pencils.size_global","text":"size_global(x::PencilArray; permute=false)\nsize_global(x::PencilArrayCollection; permute=false)\n\nGlobal dimensions associated to the given array.\n\nUnlike size, by default the returned dimensions are not permuted according to the associated pencil configuration.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#ManyPencilArray-methods-1","page":"MPI-distributed data","title":"ManyPencilArray methods","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"first(::ManyPencilArray)\ngetindex(::ManyPencilArray)\nlast(::ManyPencilArray)\nlength(::ManyPencilArray)","category":"page"},{"location":"Pencils/#Base.first-Tuple{ManyPencilArray}","page":"MPI-distributed data","title":"Base.first","text":"first(A::ManyPencilArray)\n\nReturns the first PencilArray wrapped by A.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.getindex-Tuple{ManyPencilArray}","page":"MPI-distributed data","title":"Base.getindex","text":"getindex(A::ManyPencilArray, ::Val{i})\ngetindex(A::ManyPencilArray, i::Integer)\n\nReturns the i-th PencilArray wrapped by A.\n\nIf possible, the Val{i} form should be preferred, as it is more efficient and it allows the compiler to know the return type.\n\nSee also first(::ManyPencilArray), last(::ManyPencilArray).\n\nExample\n\nA = ManyPencilArray(pencil1, pencil2, pencil3)\n\n# Get the PencilArray associated to `pencil2`.\n# u2 = A[2]\nu2 = A[Val(2)]  # faster!\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.last-Tuple{ManyPencilArray}","page":"MPI-distributed data","title":"Base.last","text":"last(A::ManyPencilArray)\n\nReturns the last PencilArray wrapped by A.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Base.length-Tuple{ManyPencilArray}","page":"MPI-distributed data","title":"Base.length","text":"length(A::ManyPencilArray)\n\nReturns the number of PencilArrays wrapped by A.\n\n\n\n\n\n","category":"method"},{"location":"Pencils/#Global-MPI-operations-1","page":"MPI-distributed data","title":"Global MPI operations","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"One of the most time-consuming parts of a large-scale computation involving multidimensional FFTs, is the global data transpositions between different MPI decomposition configurations. In Pencils, this is performed by the transpose! function, which takes two PencilArrays, typically associated to two different configurations. The implementation performs comparably to similar implementations in lower-level languages (see Benchmarks).","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"Also provided is a gather function that creates a single global array from decomposed data. This can be useful for tests (in fact, it is used in the Pencils tests to verify the correctness of the transpositions), but shouldn't be used with large datasets. It is generally useful for small problems where the global size of the data can easily fit the locally available memory.","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"transpose!\ngather","category":"page"},{"location":"Pencils/#LinearAlgebra.transpose!","page":"MPI-distributed data","title":"LinearAlgebra.transpose!","text":"transpose!(dest::PencilArray{T,N}, src::PencilArray{T,N};\n           method=TransposeMethods.IsendIrecv())\n\nTranspose data from one pencil configuration to the other.\n\nThe two pencil configurations must be compatible for transposition:\n\nthey must share the same MPI Cartesian topology,\nthey must have the same global data size,\nwhen written as a sorted tuple, the decomposed dimensions must be almost the same, with at most one difference. For instance, if the input of a 3D dataset is decomposed in (2, 3), then the output may be decomposed in (1, 3), but not in (1, 2). If the decomposed dimensions are the same, then no transposition is performed, and data is just copied if needed.\n\nThe src and dest arrays may be aliased (they can share memory space).\n\nPerformance tuning\n\nThe method argument allows to choose between transposition implementations. This can be useful to tune performance of MPI data transfers. Two values are currently accepted:\n\nTransposeMethods.IsendIrecv() uses non-blocking point-to-point data transfers (MPI_Isend and MPI_Irecv). This may be more performant since data transfers are interleaved with local data transpositions (index permutation of received data). This is the default.\nTransposeMethods.Alltoallv() uses MPI_Alltoallv for global data transpositions.\n\n\n\n\n\n","category":"function"},{"location":"Pencils/#PencilFFTs.Pencils.gather","page":"MPI-distributed data","title":"PencilFFTs.Pencils.gather","text":"gather(x::PencilArray, [root::Integer=0])\n\nGather data from all MPI processes into one (big) array.\n\nData is received by the root process.\n\nReturns the full array on the root process, and nothing on the other processes.\n\nThis can be useful for testing, but it shouldn't be used with very large datasets!\n\n\n\n\n\n","category":"function"},{"location":"Pencils/#Pencils.measuring_performance-1","page":"MPI-distributed data","title":"Measuring performance","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"It is possible to measure the time spent in different sections of the MPI data transposition routines using the TimerOutputs package. This has a (very small) performance overhead, so it is disabled by default. To enable time measurements, call TimerOutputs.enable_debug_timings(PencilFFTs.Pencils) after loading PencilFFTs. For more details see the TimerOutputs docs.","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"Minimal example:","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"using MPI\nusing PencilFFTs.Pencils\nusing TimerOutputs\n\n# Enable timing of `Pencils` functions\nTimerOutputs.enable_debug_timings(PencilFFTs.Pencils)\n\nMPI.Init()\n\npencil = Pencil(#= args... =#)\n\n# [do stuff with `pencil`...]\n\n# Retrieve and print timing data associated to `plan`\nto = get_timer(pencil)\nprint_timer(to)","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"By default, each Pencil has its own TimerOutput. If you already have a TimerOutput, you can pass it to the Pencil constructor:","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"to = TimerOutput()\npencil = Pencil(..., timer=to)\n\n# [do stuff with `pencil`...]\n\nprint_timer(to)","category":"page"},{"location":"Pencils/#Index-1","page":"MPI-distributed data","title":"Index","text":"","category":"section"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"Pages = [\"Pencils.md\"]\nOrder = [:module, :type, :constant, :function]","category":"page"},{"location":"Pencils/#","page":"MPI-distributed data","title":"MPI-distributed data","text":"[1]: Why would we want this? Perhaps because we want to efficiently perform FFTs along y, which, under this permutation, would be the fastest dimension.","category":"page"},{"location":"Transforms/#Available-transforms-1","page":"Available transforms","title":"Available transforms","text":"","category":"section"},{"location":"Transforms/#","page":"Available transforms","title":"Available transforms","text":"CurrentModule = PencilFFTs.Transforms","category":"page"},{"location":"Transforms/#","page":"Available transforms","title":"Available transforms","text":"Transforms","category":"page"},{"location":"Transforms/#PencilFFTs.Transforms","page":"Available transforms","title":"PencilFFTs.Transforms","text":"Defines different one-dimensional FFT-based transforms.\n\nThe transforms are all subtypes of an AbstractTransform type.\n\nWhen possible, the names of the transforms are kept consistent with the functions exported by AbstractFFTs.jl and FFTW.jl.\n\n\n\n\n\n","category":"module"},{"location":"Transforms/#Transform-types-1","page":"Available transforms","title":"Transform types","text":"","category":"section"},{"location":"Transforms/#","page":"Available transforms","title":"Available transforms","text":"FFT\nFFT!\nBFFT\nBFFT!\n\nRFFT\nBRFFT\n\nR2R\nR2R!\n\nNoTransform\nNoTransform!","category":"page"},{"location":"Transforms/#PencilFFTs.Transforms.FFT","page":"Available transforms","title":"PencilFFTs.Transforms.FFT","text":"FFT()\n\nComplex-to-complex FFT.\n\nSee also AbstractFFTs.fft.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.FFT!","page":"Available transforms","title":"PencilFFTs.Transforms.FFT!","text":"FFT!()\n\nIn-place version of FFT.\n\nSee also AbstractFFTs.fft!.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.BFFT","page":"Available transforms","title":"PencilFFTs.Transforms.BFFT","text":"BFFT()\n\nUnnormalised backward complex-to-complex FFT.\n\nLike AbstractFFTs.bfft, this transform is not normalised. To obtain the inverse transform, divide the output by the length of the transformed dimension.\n\nSee also AbstractFFTs.bfft.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.BFFT!","page":"Available transforms","title":"PencilFFTs.Transforms.BFFT!","text":"BFFT()\n\nIn-place version of BFFT.\n\nSee also AbstractFFTs.bfft!.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.RFFT","page":"Available transforms","title":"PencilFFTs.Transforms.RFFT","text":"RFFT()\n\nReal-to-complex FFT.\n\nSee also AbstractFFTs.rfft.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.BRFFT","page":"Available transforms","title":"PencilFFTs.Transforms.BRFFT","text":"BRFFT()\n\nUnnormalised inverse of RFFT.\n\nTo obtain the inverse transform, divide the output by the length of the transformed dimension (of the real output array).\n\nSee also AbstractFFTs.brfft.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.R2R","page":"Available transforms","title":"PencilFFTs.Transforms.R2R","text":"R2R{kind}()\n\nReal-to-real transform of type kind.\n\nThe possible values of kind are those described in the FFTW.r2r docs and the FFTW manual:\n\ndiscrete cosine transforms: FFTW.REDFT00, FFTW.REDFT01, FFTW.REDFFT10, FFTW.REDFFT11\ndiscrete sine transforms: FFTW.RODFT00, FFTW.RODFT01, FFTW.RODFFT10, FFTW.RODFFT11\ndiscrete Hartley transform: FFTW.DHT\n\nNote: half-complex format DFTs (FFTW.R2HC, FFTW.HC2R) are not currently supported.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.R2R!","page":"Available transforms","title":"PencilFFTs.Transforms.R2R!","text":"R2R!{kind}()\n\nIn-place version of R2R.\n\nSee also FFTW.r2r!.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.NoTransform","page":"Available transforms","title":"PencilFFTs.Transforms.NoTransform","text":"NoTransform()\n\nIdentity transform.\n\nSpecifies that no transformation should be applied.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.NoTransform!","page":"Available transforms","title":"PencilFFTs.Transforms.NoTransform!","text":"NoTransform!()\n\nIn-place version of NoTransform.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#Internals-1","page":"Available transforms","title":"Internals","text":"","category":"section"},{"location":"Transforms/#","page":"Available transforms","title":"Available transforms","text":"What follows is used internally by PencilFFTs.","category":"page"},{"location":"Transforms/#Types-1","page":"Available transforms","title":"Types","text":"","category":"section"},{"location":"Transforms/#","page":"Available transforms","title":"Available transforms","text":"AbstractTransform\nIdentityPlan\nIdentityPlan!","category":"page"},{"location":"Transforms/#PencilFFTs.Transforms.AbstractTransform","page":"Available transforms","title":"PencilFFTs.Transforms.AbstractTransform","text":"AbstractTransform\n\nSpecifies a one-dimensional FFT-based transform.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.IdentityPlan","page":"Available transforms","title":"PencilFFTs.Transforms.IdentityPlan","text":"IdentityPlan\n\nType of plan associated to NoTransform.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#PencilFFTs.Transforms.IdentityPlan!","page":"Available transforms","title":"PencilFFTs.Transforms.IdentityPlan!","text":"IdentityPlan!\n\nType of plan associated to NoTransform!.\n\n\n\n\n\n","category":"type"},{"location":"Transforms/#Functions-1","page":"Available transforms","title":"Functions","text":"","category":"section"},{"location":"Transforms/#","page":"Available transforms","title":"Available transforms","text":"plan\n\nbinv\nscale_factor\n\neltype_input\neltype_output\nexpand_dims\nis_inplace\nkind\nlength_output","category":"page"},{"location":"Transforms/#PencilFFTs.Transforms.plan","page":"Available transforms","title":"PencilFFTs.Transforms.plan","text":"plan(transform::AbstractTransform, A, [dims];\n     flags=FFTW.ESTIMATE, timelimit=Inf)\n\nCreate plan to transform array A along dimensions dims.\n\nIf dims is not specified, all dimensions of A are transformed.\n\nThis function wraps the AbstractFFTs.jl and FFTW.jl plan creation functions. For more details on the function arguments, see AbstractFFTs.plan_fft.\n\n\n\n\n\n","category":"function"},{"location":"Transforms/#PencilFFTs.Transforms.binv","page":"Available transforms","title":"PencilFFTs.Transforms.binv","text":"binv(transform::AbstractTransform)\n\nReturns the backwards transform associated to the given transform.\n\nThe backwards transform returned by this function is not normalised. The normalisation factor for a given array can be obtained by calling scale_factor.\n\nExample\n\njulia> binv(Transforms.FFT())\nBFFT\n\njulia> binv(Transforms.BRFFT())\nRFFT\n\n\n\n\n\n","category":"function"},{"location":"Transforms/#PencilFFTs.Transforms.scale_factor","page":"Available transforms","title":"PencilFFTs.Transforms.scale_factor","text":"scale_factor(transform::AbstractTransform, A, [dims])\n\nGet factor required to normalise the given array after a transformation along dimensions dims (all dimensions by default).\n\nThe array A must have the dimensions of the transform output.\n\nImportant: the dimensions dims must be the same that were passed to plan.\n\nExamples\n\njulia> C = zeros(ComplexF32, 3, 4, 5);\n\njulia> scale_factor(Transforms.FFT(), C)\n60\n\njulia> scale_factor(Transforms.BFFT(), C)\n60\n\njulia> scale_factor(Transforms.BFFT(), C, 2:3)\n20\n\njulia> R = zeros(Float64, 3, 4, 5);\n\njulia> scale_factor(Transforms.BRFFT(), R, 2)\n4\n\njulia> scale_factor(Transforms.BRFFT(), R, 2:3)\n20\n\nThis will fail because the output of RFFT is complex, and R is a real array:\n\njulia> scale_factor(Transforms.RFFT(), R, 2:3)\nERROR: MethodError: no method matching scale_factor(::PencilFFTs.Transforms.RFFT, ::Array{Float64,3}, ::UnitRange{Int64})\n\n\n\n\n\n","category":"function"},{"location":"Transforms/#PencilFFTs.Transforms.eltype_input","page":"Available transforms","title":"PencilFFTs.Transforms.eltype_input","text":"eltype_input(transform::AbstractTransform, real_type<:AbstractFloat)\n\nDetermine input data type for a given transform given the floating point precision of the input data.\n\nSome transforms, such as R2R and NoTransform, can take both real and complex data. For those kinds of transforms, Nothing is returned.\n\nExample\n\njulia> eltype_input(Transforms.FFT(), Float32)\nComplex{Float32}\n\njulia> eltype_input(Transforms.RFFT(), Float64)\nFloat64\n\njulia> eltype_input(Transforms.R2R{FFTW.REDFT01}(), Float64)\nNothing\n\njulia> eltype_input(Transforms.NoTransform(), Float64)\nNothing\n\n\n\n\n\n\n","category":"function"},{"location":"Transforms/#PencilFFTs.Transforms.eltype_output","page":"Available transforms","title":"PencilFFTs.Transforms.eltype_output","text":"eltype_output(transform::AbstractTransform, eltype_input)\n\nReturns the output data type for a given transform given the input type.\n\nThrows ArgumentError if the input data type is incompatible with the transform type.\n\nExample\n\njulia> eltype_output(Transforms.NoTransform(), Float32)\nFloat32\n\njulia> eltype_output(Transforms.RFFT(), Float64)\nComplex{Float64}\n\njulia> eltype_output(Transforms.BRFFT(), ComplexF32)\nFloat32\n\njulia> eltype_output(Transforms.FFT(), Float64)\nERROR: ArgumentError: invalid input data type for PencilFFTs.Transforms.FFT: Float64\n\n\n\n\n\n","category":"function"},{"location":"Transforms/#PencilFFTs.Transforms.expand_dims","page":"Available transforms","title":"PencilFFTs.Transforms.expand_dims","text":"expand_dims(transform::AbstractTransform, Val(N))\n\nExpand a single multidimensional transform into one transform per dimension.\n\nExample\n\n# Expand a real-to-complex transform in 3 dimensions.\njulia> expand_dims(Transforms.RFFT(), Val(3))\n(RFFT, FFT, FFT)\n\njulia> expand_dims(Transforms.BRFFT(), Val(3))\n(BRFFT, BFFT, BFFT)\n\njulia> expand_dims(Transforms.NoTransform(), Val(2))\n(NoTransform, NoTransform)\n\n\n\n\n\n","category":"function"},{"location":"Transforms/#PencilFFTs.Transforms.is_inplace","page":"Available transforms","title":"PencilFFTs.Transforms.is_inplace","text":"is_inplace(transform::AbstractTransform)         -> Bool\nis_inplace(transforms::Vararg{AbtractTransform}) -> Union{Bool, Nothing}\n\nCheck whether a transform or a list of transforms is performed in-place.\n\nIf the list of transforms has a combination of in-place and out-of-place transforms, nothing is returned.\n\nExample\n\njulia> is_inplace(Transforms.RFFT())\nfalse\n\njulia> is_inplace(Transforms.NoTransform!())\ntrue\n\njulia> is_inplace(Transforms.FFT!(), Transforms.R2R!{FFTW.REDFT01}())\ntrue\n\njulia> is_inplace(Transforms.FFT(), Transforms.R2R{FFTW.REDFT01}())\nfalse\n\njulia> is_inplace(Transforms.FFT(), Transforms.R2R!{FFTW.REDFT01}()) === nothing\ntrue\n\n\n\n\n\n\n","category":"function"},{"location":"Transforms/#PencilFFTs.Transforms.kind","page":"Available transforms","title":"PencilFFTs.Transforms.kind","text":"kind(transform::R2R)\n\nGet kind of real-to-real transform.\n\n\n\n\n\n","category":"function"},{"location":"Transforms/#PencilFFTs.Transforms.length_output","page":"Available transforms","title":"PencilFFTs.Transforms.length_output","text":"length_output(transform::AbstractTransform, length_in::Integer)\n\nReturns the length of the transform output, given the length of its input.\n\nThe input and output lengths are specified in terms of the respective input and output datatypes. For instance, for real-to-complex transforms, these are respectively the length of input real data and of output complex data.\n\nAlso note that for backward real-to-complex transforms (BRFFT), it is assumed that the real data length is even. See also the AbstractFFTs.irfft docs.\n\n\n\n\n\n","category":"function"},{"location":"Transforms/#Index-1","page":"Available transforms","title":"Index","text":"","category":"section"},{"location":"Transforms/#","page":"Available transforms","title":"Available transforms","text":"Pages = [\"Transforms.md\"]\nOrder = [:module, :type, :function]","category":"page"},{"location":"examples/in-place/#In-place-transforms-1","page":"In-place transforms","title":"In-place transforms","text":"","category":"section"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"Complex-to-complex and real-to-real transforms can be performed in-place, enabling important memory savings. The procedure is very similar to that of out-of-place transforms described in the tutorial. The differences are illustrated in the sections below.","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"A working implementation of this example can be found in examples/in-place.jl.","category":"page"},{"location":"examples/in-place/#Creating-plans-1","page":"In-place transforms","title":"Creating plans","text":"","category":"section"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"To create an in-place plan, pass an in-place transform such as Transforms.FFT! or Transforms.R2R! to PencilFFTPlan. For instance:","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"dims = (16, 32, 64)\n\n# Apply a 3D in-place complex-to-complex FFT.\ntransform = Transforms.FFT!()\n\n# One can also combine different types of in-place transforms.\n# For instance:\n# transform = (\n#     Transforms.R2R!{FFTW.REDFT01}(),\n#     Transforms.FFT!(),\n#     Transforms.R2R!{FFTW.DHT}(),\n# )\n\ncomm = MPI.COMM_WORLD\nNproc = MPI.Comm_size(comm)\nproc_dims = (Nproc, )  # let's perform a 1D decomposition\n\n# Create plan\nplan = PencilFFTPlan(dims, transform, proc_dims, comm)","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"Note that in-place real-to-complex transforms are not currently supported. (In other words, the RFFT! transform type is not defined.)","category":"page"},{"location":"examples/in-place/#Allocating-data-1","page":"In-place transforms","title":"Allocating data","text":"","category":"section"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"As with out-of-place plans, data should be allocated using allocate_input. The difference is that, for in-place plans, this function returns a ManyPencilArray object, which is a container holding multiple PencilArray views sharing the same memory space.","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"# Allocate data for the plan.\n# Since `plan` is in-place, this returns a `ManyPencilArray` container.\nA = allocate_input(plan)","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"Note that allocate_output also works for in-place plans, but it returns exactly the same thing as allocate_input.","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"As shown in the next section, in-place plans must be applied on the returned ManyPencilArray. On the other hand, one usually wants to access and modify data, and for this one needs the PencilArray views contained in the ManyPencilArray. The input and output array views can be obtained by calling first(::ManyPencilArray) and last(::ManyPencilArray).","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"For instance, we can initialise the input array with some data before transforming:","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"using Random\nu_in = first(A)  # input data view\nrandn!(u_in)","category":"page"},{"location":"examples/in-place/#Applying-plans-1","page":"In-place transforms","title":"Applying plans","text":"","category":"section"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"Like in FFTW.jl, one can perform in-place transforms using the * and \\ operators. As mentioned above, in-place plans must be applied on the ManyPencilArray containers returned by allocate_input.","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"plan * A  # perform in-place forward transform","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"After performing an in-place transform, we usually want to do operations on the output data. For instance, let's compute the global sum of the transformed data:","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"u_out = last(A)         # output data view\nsum_local = sum(u_out)  # sum of transformed data on local MPI process\nsum_global = MPI.Allreduce(sum_local, +, comm)","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"Finally, we can perform a backward transform and do stuff with the input view:","category":"page"},{"location":"examples/in-place/#","page":"In-place transforms","title":"In-place transforms","text":"plan \\ A  # perform in-place backward transform\n\n# Now we can again do stuff with the input view `u_in`...","category":"page"},{"location":"#PencilFFTs.jl-1","page":"Home","title":"PencilFFTs.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"CurrentModule = PencilFFTs","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Fast Fourier transforms of MPI-distributed Julia arrays.","category":"page"},{"location":"#Introduction-1","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package provides functionality to distribute multidimensional arrays among MPI processes, and to perform multidimensional FFTs (and related transforms) on them.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The name of this package originates from the decomposition of 3D domains along two out of three dimensions, sometimes called pencil decomposition. This is illustrated by the figure below,[1] where each coloured block is managed by a different MPI process. Typically, one wants to compute FFTs on a scalar or vector field along the three spatial dimensions. In the case of a pencil decomposition, 3D FFTs are performed one dimension at a time, along the non-decomposed direction. Transforms must then be interleaved with global data transpositions to switch between pencil configurations. In high-performance computing environments, such data transpositions are generally the most expensive part of a parallel FFT computation, due to the large cost of communications between computing nodes.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"<div class=\"figure\">\n  <img\n    width=\"85%\"\n    src=\"img/pencils.svg\"\n    alt=\"Pencil decomposition of 3D domains\">\n</div>","category":"page"},{"location":"#","page":"Home","title":"Home","text":"More generally, PencilFFTs allows to decompose and perform FFTs on geometries of arbitrary dimension N. The decompositions can be performed along an arbitrary number M  N of dimensions.[2] Moreover, the transforms applied along each dimension can be arbitrarily chosen (and combined) among those supported by FFTW.jl, including complex-to-complex, real-to-complex and real-to-real transforms.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The generic and efficient implementation of this package is greatly enabled by the use of zero-cost abstractions in Julia. As shown in the Benchmarks section, PencilFFTs scales well to large numbers of processes, and performs similarly to the Fortran implementation of P3DFFT, possibly the most popular library for computing parallel FFTs using 2D domain decomposition.","category":"page"},{"location":"#Installation-1","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"PencilFFTs can be installed using the Julia package manager:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"julia> ] add PencilFFTs","category":"page"},{"location":"#Similar-projects-1","page":"Home","title":"Similar projects","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"FFTW3 implements distributed-memory transforms using MPI, but these are limited to 1D decompositions. Also, this functionality is not currently included in the FFTW.jl wrappers.\nPFFT is a very general parallel FFT library written in C.\nP3DFFT implements parallel 3D FFTs using pencil decomposition in Fortran and C++.\n2DECOMP&FFT is another parallel 3D FFT library using pencil decomposition written in Fortran.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"[1]: Figure adapted from this thesis.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"[2]: For the pencil decomposition represented in the figure, N = 3 and M = 2.","category":"page"},{"location":"tutorial/#Tutorial-1","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"CurrentModule = PencilFFTs","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The following tutorial shows how to perform a 3D FFT of real periodic data defined on a grid of N_x  N_y  N_z points.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"<div class=\"figure\">\n  <!--\n  Note: this is evaluated from the directory where the Tutorial page is\n  built. This directory varies depending on whether `prettyurls` is enabled in\n  `makedocs`. Here we assume `prettyurls=true`.\n  -->\n  <img\n    width=\"85%\"\n    src=\"../img/pencils.svg\"\n    alt=\"Pencil decomposition of 3D domains\">\n</div>","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The example assumes that 12 MPI processes are available. The data is to be distributed on a 2D MPI topology of dimensions 3  4, as represented in the above figure.","category":"page"},{"location":"tutorial/#Creating-plans-1","page":"Tutorial","title":"Creating plans","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The first thing to do is to create a PencilFFTPlan, which requires information on the global dimensions N_x  N_y  N_z of the data, on the transforms that will be applied, and on the way the data is distributed among MPI processes (i.e. number of processes along each dimension):","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using MPI\nusing PencilFFTs\n\nMPI.Init()\n\n# Input data dimensions (Nx × Ny × Nz)\ndims = (16, 32, 64)\n\n# Apply a 3D real-to-complex (r2c) FFT.\ntransform = Transforms.RFFT()\n\n# For more control, one can instead separately specify the transforms along each dimension:\n# transform = (Transforms.RFFT(), Transforms.FFT(), Transforms.FFT())\n\n# MPI topology information\ncomm = MPI.COMM_WORLD  # we assume MPI.Comm_size(comm) == 12\nproc_dims = (3, 4)     # 3 processes along `y`, 4 along `z`\n\n# Create plan\nplan = PencilFFTPlan(dims, transform, proc_dims, comm)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"See the PencilFFTPlan constructor for details on the accepted options, and the Transforms module for the possible transforms. It is also possible to enable fine-grained performance measurements via the TimerOutputs package, as described in Measuring performance.","category":"page"},{"location":"tutorial/#Allocating-data-1","page":"Tutorial","title":"Allocating data","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Next, we want to apply the plan on some data. Transforms may only be applied on PencilArrays, which are array wrappers that include MPI decomposition information (in some sense, analogous to DistributedArrays in Julia's distributed computing approach). The helper function allocate_input can be used to allocate a PencilArray that is compatible with our plan:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"# In our example, this returns a 3D PencilArray of real data (Float64).\nu = allocate_input(plan)\n\n# Fill the array with some (random) data\nusing Random\nrandn!(u)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"PencilArrays are a subtype of AbstractArray, and thus they support all common array operations.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Similarly, to preallocate output data, one can use allocate_output:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"# In our example, this returns a 3D PencilArray of complex data (Complex{Float64}).\nv = allocate_output(plan)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"This is only required if one wants to apply the plans using a preallocated output (with mul!, see right below).","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The data types returned by allocate_input and allocate_output are slightly different when working with in-place transforms. See the in-place example for details.","category":"page"},{"location":"tutorial/#Applying-plans-1","page":"Tutorial","title":"Applying plans","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The interface to apply plans is consistent with that of AbstractFFTs. Namely, * and mul! are respectively used for forward transforms without and with preallocated output data. Similarly, \\ and ldiv! are used for backward transforms.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using LinearAlgebra  # for mul!, ldiv!\n\n# Apply plan on `u` with `v` as an output\nmul!(v, plan, u)\n\n# Apply backward plan on `v` with `w` as an output\nw = similar(u)\nldiv!(w, plan, v)  # now w ≈ u","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Note that, consistently with AbstractFFTs, normalisation is performed at the end of a backward transform, so that the original data is recovered when applying a forward followed by a backward transform.","category":"page"},{"location":"tutorial/#Accessing-and-modifying-data-1","page":"Tutorial","title":"Accessing and modifying data","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"For any given MPI process, a PencilArray holds the data associated to its local partition in the global geometry. PencilArrays are accessed using local indices that start at 1, regardless of the location of the local process in the MPI topology. Note that PencilArrays, being based on regular Arrays, support both linear and Cartesian indexing (see the Julia docs for details).","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"For convenience, the global_view function can be used to generate an OffsetArray wrapper that takes global indices.","category":"page"},{"location":"tutorial/#tutorial:output_data_layout-1","page":"Tutorial","title":"Output data layout","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In memory, the dimensions of the transform output are by default reversed with respect to the input. That is, if the order of indices in the input data is (x, y, z), then the output has order (z, y, x) in memory. This detail is hidden from the user, and output arrays are always accessed in the same order as the input data, regardless of the underlying output dimension permutation. This applies to PencilArrays and to OffsetArrays returned by global_view.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The reasoning behind dimension permutations, is that they allow to always perform FFTs along the fastest array dimension and to avoid a local data transposition, resulting in performance gains. A similar approach is followed by other parallel FFT libraries. FFTW itself, in its distributed-memory routines, includes a flag that enables a similar behaviour. In PencilFFTs, index permutation is the default, but it can be disabled via the permute_dims flag of PencilFFTPlan.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"A great deal of work has been spent in making generic index permutations as efficient as possible, both in intermediate and in the output state of the multidimensional transforms. This has been achieved, in part, by making sure that permutations such as (3, 2, 1) are compile-time constants (using value types).","category":"page"},{"location":"tutorial/#Further-reading-1","page":"Tutorial","title":"Further reading","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The examples on the sidebar further illustrate the use of transforms and provide an introduction to working with MPI-distributed data in the form of PencilArrays.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"In addition to the examples, some useful scripts are available in the test/ directory of the PencilFFTs repo. In particular, the test/taylor_green.jl example is a (very simple) fluid dynamics application around the Taylor–Green vortex flow.","category":"page"},{"location":"PencilFFTs/#Distributed-FFTs-1","page":"Distributed FFTs","title":"Distributed FFTs","text":"","category":"section"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"Distributed FFTs are built on top of the Pencils and the Transforms modules, and are implemented in the PencilFFTs module.","category":"page"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"CurrentModule = PencilFFTs","category":"page"},{"location":"PencilFFTs/#Types-1","page":"Distributed FFTs","title":"Types","text":"","category":"section"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"PencilFFTPlan","category":"page"},{"location":"PencilFFTs/#PencilFFTs.PencilFFTPlan","page":"Distributed FFTs","title":"PencilFFTs.PencilFFTPlan","text":"PencilFFTPlan{T,N,M}\n\nPlan for N-dimensional FFT-based transform on MPI-distributed data.\n\n\n\nPencilFFTPlan(size_global::Dims{N}, transforms,\n              proc_dims::Dims{M}, comm::MPI.Comm, [real_type=Float64];\n              extra_dims=(),\n              fftw_flags=FFTW.ESTIMATE, fftw_timelimit=FFTW.NO_TIMELIMIT,\n              permute_dims=Val(true),\n              transpose_method=TransposeMethods.IsendIrecv(),\n              timer=TimerOutput(),\n              )\n\nCreate plan for N-dimensional transform.\n\nsize_global specifies the global dimensions of the input data.\n\ntransforms should be a tuple of length N specifying the transforms to be applied along each dimension. Each element must be a subtype of Transforms.AbstractTransform. For all the possible transforms, see Transform types. Alternatively, transforms may be a single transform that will be automatically expanded into N equivalent transforms. This is illustrated in the example below.\n\nThe transforms are applied one dimension at a time, with the leftmost dimension first for forward transforms. For multidimensional transforms of real data, this means that a real-to-complex transform must be performed along the first dimension, and then complex-to-complex transforms are performed along the other two dimensions (see example below).\n\nThe data is distributed over the MPI processes in the comm communicator. The distribution is performed over M dimensions (with M < N) according to the values in proc_dims, which specifies the number of MPI processes to put along each dimension.\n\nOptional arguments\n\nThe floating point precision can be selected by setting real_type parameter, which is Float64 by default.\nThe keyword arguments fftw_flags and fftw_timelimit are passed to the FFTW plan creation functions (see AbstractFFTs docs).\npermute_dims determines whether the indices of the output data should be reversed. For instance, if the input data has global dimensions (Nx, Ny, Nz), then the output of a complex-to-complex FFT would have dimensions (Nz, Ny, Nx). This enables FFTs to always be performed along the first (i.e. fastest) array dimension, which could lead to performance gains. This option is enabled by default. For type inference reasons, it must be a value type (Val(true) or Val(false)).\ntranspose_method allows to select between implementations of the global data transpositions. See the transpose! docs for details.\ntimer should be a TimerOutput object. See Measuring performance for details.\n\nExperimental arguments\n\nThe following options are experimental and may disappear in the future.\n\nextra_dims may be used to specify the sizes of one or more extra dimensions that should not be transformed. These dimensions will be added to the rightmost (i.e. slowest) indices of the arrays. Extra dimensions can be used to contain, for instance, multiple vector field components in a single array. See Pencils.PencilArray for more details.\nNOTE: An alternative to extra_dims is to create a tuple or an array of PencilArrays, e.g. one per vector component. This can be done via the allocate_input and allocate_output functions.\n\nExample\n\nSuppose we want to perform a 3D transform of real data. The data is to be decomposed along two dimensions, over 8 MPI processes:\n\nsize_global = (64, 32, 128)  # size of real input data\n\n# Perform real-to-complex transform along the first dimension, then\n# complex-to-complex transforms along the other dimensions.\ntransforms = (Transforms.RFFT(), Transforms.FFT(), Transforms.FFT())\n# transforms = Transforms.RFFT()  # this is equivalent to the above line\n\nproc_dims = (4, 2)  # 2D decomposition\ncomm = MPI.COMM_WORLD\n\nplan = PencilFFTPlan(size_global, transforms, proc_dims, comm)\n\n\n\n\n\n","category":"type"},{"location":"PencilFFTs/#Functions-1","page":"Distributed FFTs","title":"Functions","text":"","category":"section"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"allocate_input(::PencilFFTPlan)\nallocate_output(::PencilFFTPlan)\nget_comm(::PencilFFTPlan)\nget_scale_factor(::PencilFFTPlan)\nget_timer(::PencilFFTPlan)\nis_inplace(::PencilFFTPlan)","category":"page"},{"location":"PencilFFTs/#PencilFFTs.allocate_input-Tuple{PencilFFTPlan}","page":"Distributed FFTs","title":"PencilFFTs.allocate_input","text":"allocate_input(p::PencilFFTPlan)          -> PencilArray\nallocate_input(p::PencilFFTPlan, dims...) -> Array{PencilArray}\nallocate_input(p::PencilFFTPlan, Val(N))  -> NTuple{N, PencilArray}\n\nAllocate uninitialised PencilArray that can hold input data for the given plan.\n\nThe second and third forms respectively allocate an array of PencilArrays of size dims, and a tuple of N PencilArrays.\n\nnote: In-place plans\nIf p is an in-place plan, a ManyPencilArray is allocated. This type holds PencilArray wrappers for the input and output transforms (as well as for intermediate transforms) which share the same space in memory. The input and output PencilArrays should be respectively accessed by calling first(::ManyPencilArray) and last(::ManyPencilArray).ExampleSuppose p is an in-place PencilFFTPlan. Then,@assert is_inplace(p)\nA = allocate_input(p) :: ManyPencilArray\nv_in = first(A)       :: PencilArray  # input data view\nv_out = last(A)       :: PencilArray  # output data viewAlso note that in-place plans must be performed directly on the returned ManyPencilArray, and not on the contained PencilArray views:p * A       # perform forward transform in-place\np \\ A       # perform backward transform in-place\n# p * v_in  # not allowed!!\n\n\n\n\n\n","category":"method"},{"location":"PencilFFTs/#PencilFFTs.allocate_output-Tuple{PencilFFTPlan}","page":"Distributed FFTs","title":"PencilFFTs.allocate_output","text":"allocate_output(p::PencilFFTPlan)          -> PencilArray\nallocate_output(p::PencilFFTPlan, dims...) -> Array{PencilArray}\nallocate_output(p::PencilFFTPlan, Val(N))  -> NTuple{N, PencilArray}\n\nAllocate uninitialised PencilArray that can hold output data for the given plan.\n\nIf p is an in-place plan, a ManyPencilArray is allocated.\n\nSee allocate_input for details.\n\n\n\n\n\n","category":"method"},{"location":"PencilFFTs/#PencilFFTs.Pencils.MPITopologies.get_comm-Tuple{PencilFFTPlan}","page":"Distributed FFTs","title":"PencilFFTs.Pencils.MPITopologies.get_comm","text":"get_comm(p::PencilFFTPlan)\n\nGet MPI communicator associated to a PencilFFTPlan.\n\n\n\n\n\n","category":"method"},{"location":"PencilFFTs/#PencilFFTs.get_scale_factor-Tuple{PencilFFTPlan}","page":"Distributed FFTs","title":"PencilFFTs.get_scale_factor","text":"get_scale_factor(p::PencilFFTPlan) :: Int\n\nGet scale factor associated to a PencilFFTPlan.\n\n\n\n\n\n","category":"method"},{"location":"PencilFFTs/#PencilFFTs.Pencils.get_timer-Tuple{PencilFFTPlan}","page":"Distributed FFTs","title":"PencilFFTs.Pencils.get_timer","text":"get_timer(p::PencilFFTPlan)\n\nGet TimerOutput attached to a PencilFFTPlan.\n\nSee Measuring performance for details.\n\n\n\n\n\n","category":"method"},{"location":"PencilFFTs/#PencilFFTs.Transforms.is_inplace-Tuple{PencilFFTPlan}","page":"Distributed FFTs","title":"PencilFFTs.Transforms.is_inplace","text":"Transforms.is_inplace(p::PencilFFTPlan)\n\nReturns true if the given plan operates in-place on the input data, false otherwise.\n\n\n\n\n\n","category":"method"},{"location":"PencilFFTs/#PencilFFTs.measuring_performance-1","page":"Distributed FFTs","title":"Measuring performance","text":"","category":"section"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"It is possible to measure the time spent in different sections of the distributed transforms using the TimerOutput package. This has a (very small) performance overhead, so it is disabled by default. To enable time measurements, call TimerOutputs.enable_debug_timings(PencilFFTs) and TimerOutputs.enable_debug_timings(PencilFFTs.Pencils) after loading PencilFFTs. For more details see the TimerOutput docs.","category":"page"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"Minimal example:","category":"page"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"using MPI\nusing PencilFFTs\nusing TimerOutputs\n\n# Enable timing of `PencilFFTs` functions\nTimerOutputs.enable_debug_timings(PencilFFTs)\nTimerOutputs.enable_debug_timings(PencilFFTs.Pencils)\n\nMPI.Init()\n\nplan = PencilFFTPlan(#= args... =#)\n\n# [do stuff with `plan`...]\n\n# Retrieve and print timing data associated to `plan`\nto = get_timer(plan)\nprint_timer(to)","category":"page"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"By default, each PencilFFTPlan has its own TimerOutput. If you already have a TimerOutput, you can pass it to the PencilFFTPlan constructor:","category":"page"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"to = TimerOutput()\nplan = PencilFFTPlan(..., timer=to)\n\n# [do stuff with `plan`...]\n\nprint_timer(to)","category":"page"},{"location":"PencilFFTs/#Internals-1","page":"Distributed FFTs","title":"Internals","text":"","category":"section"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"GlobalFFTParams","category":"page"},{"location":"PencilFFTs/#PencilFFTs.GlobalFFTParams","page":"Distributed FFTs","title":"PencilFFTs.GlobalFFTParams","text":"GlobalFFTParams{T, N, inplace}\n\nSpecifies the global parameters for an N-dimensional distributed transform. These include the element type T and global data sizes of input and output data, as well as the transform types to be performed along each dimension.\n\n\n\nGlobalFFTParams(size_global, transforms, [real_type=Float64])\n\nDefine parameters for N-dimensional transform.\n\ntransforms must be a tuple of length N specifying the transforms to be applied along each dimension. Each element must be a subtype of Transforms.AbstractTransform. For all the possible transforms, see Transform types.\n\nThe element type must be a real type accepted by FFTW, i.e. either Float32 or Float64.\n\nNote that the transforms are applied one dimension at a time, with the leftmost dimension first for forward transforms.\n\nExample\n\nTo perform a 3D FFT of real data, first a real-to-complex FFT must be applied along the first dimension, followed by two complex-to-complex FFTs along the other dimensions:\n\njulia> size_global = (64, 32, 128);  # size of real input data\n\njulia> transforms = (Transforms.RFFT(), Transforms.FFT(), Transforms.FFT());\n\njulia> fft_params = PencilFFTs.GlobalFFTParams(size_global, transforms);\n\n\n\n\n\n","category":"type"},{"location":"PencilFFTs/#Index-1","page":"Distributed FFTs","title":"Index","text":"","category":"section"},{"location":"PencilFFTs/#","page":"Distributed FFTs","title":"Distributed FFTs","text":"Pages = [\"PencilFFTs.md\"]\nOrder = [:module, :type, :function]","category":"page"}]
}
